```python
1. Markov's inequality: Let \( X \) be a discrete random variable taking non-negative values with a finite mean \( \mu \). Then,
\[ P(X \ge c) \le \frac{\mu}{c} \]

2. Chebyshev's inequality: Let \( X \) be a discrete random variable with a finite mean \( \mu \) and a finite variance \( \sigma^2 \). Then,
\[ P(|X - \mu| \ge k\sigma) \le \frac{1}{k^2} \]

3. Weak Law of Large numbers: Let \( X_1, X_2, \ldots, X_n \sim \) iid \( X \) with \( E[X] = \mu \), Var(X) = \( \sigma^2 \).
Define sample mean \( \bar{X} = \frac{X_1 + X_2 + \cdots + X_n}{n} \). Then,
\[ P(|\bar{X} - \mu| > \delta) \le \frac{\sigma^2}{n\delta^2} \]

4. Using CLT to approximate probability: Let \( X_1, X_2, \ldots, X_n \sim \) iid \( X \) with \( E[X] = \mu \), Var(X) = \( \sigma^2 \).
Define \( Y = X_1 + X_2 + \cdots + X_n \). Then,
\[ \frac{Y - n\mu}{\sqrt{n\sigma^2}} \approx \text{Normal}(0, 1) \]

5. Bias of an estimator: Bias(\(\hat{\theta}, \theta\)) = \( E[\hat{\theta}] - \theta \).

6. Method of moments: Sample moments, \( M_k(X_1, X_2, \ldots, X_n) = \frac{1}{n} \sum_{i=1}^n X_i^k \)
Procedure: For one parameter \( \theta \)

  - Sample moment: \( m_1 \)
  - Distribution moment: \( E(X) = f(\theta) \)
  - Solve for \( \theta \) from \( f(\theta) = m_1 \) in terms of \( m_1 \).
  - \( \hat{\theta} \): replace \( m_1 \) by \( M_1 \) in the above solution.

7. Likelihood of i.i.d. samples: Likelihood of a sampling \( x_1, x_2, \ldots, x_n \), denoted
\[ L(x_1, \ldots, x_n) = \prod_{i=1}^n f_X(x_i; \theta_1, \theta_2, \ldots) \]

8. Maximum likelihood (ML) estimation:
\[ \hat{\theta}_1, \hat{\theta}_2, \ldots = \arg \max_{\theta_1, \theta_2, \ldots} \prod_{i=1}^n f_X(x_i; \theta_1, \theta_2, \ldots) \]
```