```plaintext
1. Markov’s inequality: Let \( X \) be a discrete random variable taking non-negative values with a finite mean \( \mu \). Then,
\[ P(X \ge c) \le \frac{\mu}{c} \]

2. Chebyshev’s inequality: Let \( X \) be a discrete random variable with a finite mean \( \mu \) and a finite variance \( \sigma^2 \). Then,
\[ P(|X - \mu| \ge k\sigma) \le \frac{1}{k^2} \]

3. Weak Law of Large numbers: Let \( X_1, X_2, \ldots, X_n \sim \text{iid } X \) with \( E[X] = \mu, \text{ Var}(X) = \sigma^2 \).
Define sample mean \(\overline{X} = \frac{X_1 + X_2 + \ldots + X_n}{n} \). Then,
\[ P(|\overline{X} - \mu| > \delta) \le \frac{\sigma^2}{n\delta^2} \]

4. Using CLT to approximate probability: Let \( X_1, X_2, \ldots, X_n \sim \text{iid } X \) with \( E[X] = \mu, \text{ Var}(X) = \sigma^2 \).
Define \( Y = X_1 + X_2 + \ldots + X_n \). Then,
\[ \frac{Y - n\mu}{\sqrt{n}\sigma} \approx \text{Normal}(0,1) \]

5. Bias of an estimator: \( \text{Bias}(\widehat{\theta}, \theta) = E[\widehat{\theta}] - \theta \).

6. Method of moments: Sample moments, \( M_k(X_1, X_2, \ldots, X_n) = \frac{1}{n}\sum_{i=1}^{n}X_i^k \)
   - **Procedure:** For one parameter \( \theta \)
     - Sample moment: \( m_1 \)
     - Distribution moment: \( E(X) = f(\theta) \)
     - Solve for \( \theta \) from \( f(\theta) = m_1 \) in terms of \( m_1 \).
     - \( \widehat{\theta} \): replace \( m_1 \) by \( M_1 \) in the above solution.

7. Likelihood of i.i.d. samples: Likelihood of a sampling \( x_1, x_2, \ldots, x_n \), denoted
\[ L(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} f_X(x_i; \theta_1, \theta_2, \ldots) \]

8. Maximum likelihood (ML) estimation:
\[ \widehat{\theta_1}, \widehat{\theta_2}, \ldots = \arg \max_{\theta_1, \theta_2, \ldots} \prod_{i=1}^{n} f_X(x_i; \theta_1, \theta_2, \ldots) \]

9. Bayesian estimation: Let \( X_1, X_2, \ldots, X_n \sim \text{i.i.d. } X, \text{ parameter } \Theta \).
   - Prior distribution of \( \Theta \): \( \Theta \sim f_{\Theta}(\theta) \)
   - Samples, \( S : (X_1 = x_1, \ldots, X_n = x_n) \)
   - Posterior: \( \Theta | (X_1 = x_1, \ldots, X_n = x_n) \)
   - Bayes’ rule: Posterior \( \propto \text{Prior } \times \text{Likelihood} \)
   - Posterior density \( \propto f_{\Theta}(\theta) \times P(X_1 = x_1, \ldots, X_n = x_n | \Theta = \theta) \)

10. Normal samples with unknown mean and known variance:
    \( X_1, \ldots, X_n \sim \text{i.i.d. Normal}(\mu, \sigma^2) \).
    Prior \( M \sim \text{Normal}(\mu_0, \sigma_0^2) \).
    Posterior mean:
\[ \hat{\mu} = \overline{X}\left(\frac{\sigma_0^2}{n\sigma^2 + \sigma_0^2}\right) + \mu_0\left(\frac{\sigma^2}{n\sigma_0^2 + \sigma^2}\right) \]
```