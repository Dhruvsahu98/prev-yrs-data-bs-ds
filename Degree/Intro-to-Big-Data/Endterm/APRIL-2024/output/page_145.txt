```python
clicks = (
    ...
    spark
    .readStream
    .format("kafka")
    .option("subscribe", "clicks")
    ...
    .load()
)
clicks
    .join(pages, "pageid")
    .groupBy("pageName")
    .count()
```

Your goal is to optimize the code to bring down execution of each iteration within 1 minute. Which of the following represent options that will help in this mission?

**Options :**

6406532755194. ❌ Convert to batch and use cron to schedule for every 1 minute since streaming computation has more overhead than batch processing

6406532755195. ✅ Ensure "pages" dataframe is broadcast before the streaming begins so that each executor has access to its own copy without having to reshuffle

Rewrite the join code as follows
```python
clicks
    .groupBy("pageId")
    .count()
    .join(pages, "pageid")
    .select("pagename", "count")
```

6406532755196. ❌

Rewrite the join code as follows
```python
pages
    .join(clicks, "pageId")
    .groupBy("pageName")
    .count()
```

6406532755197. ❌